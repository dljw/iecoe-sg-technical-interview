{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJAfljrxmgxC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkgXKGrhv325"
   },
   "source": [
    "## Loading and preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fnxpUZI7v8Vz",
    "outputId": "eea14904-d1c7-4f63-c8fb-5721ca20d356"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8qIsaF-v5_n"
   },
   "outputs": [],
   "source": [
    "fake = pd.read_csv('drive/MyDrive/SAP test/Fake.csv')\n",
    "real = pd.read_csv('drive/MyDrive/SAP test/True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E2RB7uCxwEpV",
    "outputId": "1d58655d-5152-4269-afc6-7586dfeb6ccd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23481, 4) (21417, 4)\n"
     ]
    }
   ],
   "source": [
    "print(fake.shape,real.shape)\n",
    "#No imbalance of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frk6FjaJwKaa"
   },
   "outputs": [],
   "source": [
    "real['real'] = 1\n",
    "fake['real'] = 0\n",
    "df = pd.concat([real,fake])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ru8vCP2jqOg"
   },
   "source": [
    "Data cleaning codes taken from [here](https://www.kaggle.com/madz2000/nlp-using-glove-embeddings-99-87-accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRMvUSZozLDj"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install BeautifulSoup4\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "# Removing URL's\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop_words:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "df['text']=df['text'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IyYc4lxwPHK"
   },
   "source": [
    "Since the purpose of this test is to access my ability to apply NLP modelling techniques, I will be using only the \"text\" column which consist the main bulk of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "urweQR1wzuNt"
   },
   "outputs": [],
   "source": [
    "x = df['text']\n",
    "y = df['real']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kjq9v3sqzxol",
    "outputId": "9826499a-e5c3-4b3a-892d-0657c3acdf88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40408,) (40408,) (4490,) (4490,) (4490,) (4490,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1,random_state=123)\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(x,y, test_size=0.1,random_state=123)\n",
    "\n",
    "print(x_train.shape,y_train.shape,x_val.shape,y_val.shape,x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ap81peLHyrV2"
   },
   "source": [
    "## Analysing the problem and choosing text processing technique\n",
    "\n",
    "News constantly evolve overtime and the context of news changes as well. For example, in the year 2019, no one knows about COVID-19, but in the following year, news of COVID-19 appears everywhere. Because of this, it is important for us to constantly check and update the model with new dataset so that it is able to keep up with trends.\n",
    "\n",
    "To minimise the impact of changing trend, models like Tfidf or bag-of-words that solely tries to match documents based on words occurance might give high results in training and test set but when it comes to new unknown data a year later, the accuracy might drop significantly. Additionally, it might also result in a Out-of-Vocab problem.\n",
    "\n",
    "Because of this limitation and the context of the problem, I will be exploring the following word embedding methods that uses words dictionary on a global scale rather than just limiting on the local dataset. Word embedding methods also places emphases on the semantic relationships between words instead of solely relying on just the frequency. These are the following word embedding methods that I have shortlisted:\n",
    "\n",
    "1. GloVe\n",
    "2. Word2Vec\n",
    "3. CountVectorization\n",
    "\n",
    "The last on the list is a not a word embedding method but a frequency based embedding method. I picked on non-word embedding method to get a broader understanding of how each method would perform in this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2a_lxxvztyl"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def f1scores(y_true,y_pred):\n",
    "  print(\"F1 scores {}\".format(metrics.f1_score(y_true,y_pred)))\n",
    "\n",
    "def accuracyScores(y_true, y_pred):\n",
    "  print(\"Accuracy Scores {}\".format(metrics.accuracy_score(y_true,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKVMqPn8yqdE"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers, initializers, optimizers, callbacks\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,LSTM,Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eC8Qm1n3OTO"
   },
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1nijpkp38_Z"
   },
   "outputs": [],
   "source": [
    "## Run this code to download the glove embedding file\n",
    "# import zipfile\n",
    "\n",
    "# !wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "# zip_ref = zipfile.ZipFile('glove.twitter.27B.zip', 'r')\n",
    "# zip_ref.extractall('')\n",
    "# zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1d3peBJv3Jd6"
   },
   "outputs": [],
   "source": [
    "embedding_file = 'drive/MyDrive/SAP test/glove.twitter.27B.100d.txt'\n",
    "\n",
    "def get_coefs(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(embedding_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THQE6GV04PCQ"
   },
   "outputs": [],
   "source": [
    "max_features = 10000 ##Will take into account of only the top 10000 most common words\n",
    "maxlen = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGsaWXEP4bYJ"
   },
   "outputs": [],
   "source": [
    "#Tokenizing the text column\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "tokenized_train_text = tokenizer.texts_to_sequences(x_train)\n",
    "x_train_text = pad_sequences(tokenized_train_text, maxlen=maxlen)\n",
    "\n",
    "tokenized_val_text = tokenizer.texts_to_sequences(x_val)\n",
    "x_val_text = pad_sequences(tokenized_val_text, maxlen=maxlen)\n",
    "\n",
    "tokenized_test_text = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_text = pad_sequences(tokenized_test_text, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Ec2Q2GA5JgZ",
    "outputId": "7de5317b-80f9-492d-ac3b-ab69f80952cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "#change below line if computing normal stats is too slow\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    #Retrieve weights of words that appeared in the text column \n",
    "    embedding_vector = embeddings_index.get(word) \n",
    "    \n",
    "    ##Add the weights to the embedding matrix, if it exist inside the embedding_index\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFcEihhP6ebU"
   },
   "source": [
    "Creating the model with Weights given by GloVe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOf6XgAxGikD"
   },
   "outputs": [],
   "source": [
    "##Codes taken from  https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpHwaDPC5LTI",
    "outputId": "c17877bb-16aa-43b7-ff13-557e92381dca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 5\n",
    "embed_size = 100\n",
    "\n",
    "##Training the Model\n",
    "#Defining Neural Network\n",
    "model = Sequential()\n",
    "#Non-trainable embeddidng layer with weights taken from the GloVe dataset\n",
    "model.add(Embedding(max_features, output_dim=embed_size, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
    "#LSTM \n",
    "model.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\n",
    "model.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\n",
    "model.add(Dense(units = 32 , activation = 'relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['accuracy',f1_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vtFAKsP7CGB",
    "outputId": "d0892191-4001-41c1-a907-a0ec25772ffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 300, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 300, 128)          117248    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,168,769\n",
      "Trainable params: 168,769\n",
      "Non-trainable params: 1,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-tm8tUo7EUy",
    "outputId": "2bf19ee6-466b-4ea6-f54b-94258bb00b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "158/158 [==============================] - 322s 2s/step - loss: 0.2764 - accuracy: 0.8756 - f1_m: 0.8491 - val_loss: 0.0757 - val_accuracy: 0.9771 - val_f1_m: 0.9748\n",
      "Epoch 2/5\n",
      "158/158 [==============================] - 317s 2s/step - loss: 0.0372 - accuracy: 0.9892 - f1_m: 0.9887 - val_loss: 0.0115 - val_accuracy: 0.9958 - val_f1_m: 0.9952\n",
      "Epoch 3/5\n",
      "158/158 [==============================] - 317s 2s/step - loss: 0.0145 - accuracy: 0.9951 - f1_m: 0.9948 - val_loss: 0.0137 - val_accuracy: 0.9951 - val_f1_m: 0.9946\n",
      "Epoch 4/5\n",
      "158/158 [==============================] - 314s 2s/step - loss: 0.0105 - accuracy: 0.9967 - f1_m: 0.9966 - val_loss: 0.0138 - val_accuracy: 0.9960 - val_f1_m: 0.9956\n",
      "Epoch 5/5\n",
      "158/158 [==============================] - 320s 2s/step - loss: 0.0099 - accuracy: 0.9965 - f1_m: 0.9964 - val_loss: 0.0253 - val_accuracy: 0.9920 - val_f1_m: 0.9913\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_text, y_train, batch_size = batch_size , validation_data = (x_val_text,y_val) , epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TMBgoAmnF5Em",
    "outputId": "2a12ce6e-f969-4de4-b5cd-5befb3d4df64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 18s 127ms/step - loss: 0.0253 - accuracy: 0.9920 - f1_m: 0.9908\n"
     ]
    }
   ],
   "source": [
    "_, accuracy_gl, f1_score_gl = model.evaluate(x=x_test_text, y= y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlUHWvaVGS-R",
    "outputId": "43b884c7-098d-49c6-8566-4c83ba7f98fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe: Accuracy score   0.9919821619987488\n",
      "GloVe: F1 score 0.990760087966919\n"
     ]
    }
   ],
   "source": [
    "print(\"GloVe: Accuracy score   {}\".format(accuracy_gl))\n",
    "print(\"GloVe: F1 score {}\".format(f1_score_gl))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3SYLoxqMnyg"
   },
   "source": [
    "**word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XS_-gDG-SVfB",
    "outputId": "c9286924-8d25-46d8-a6db-61326e6a98a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive  sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tabjUUHVSgyw",
    "outputId": "1907602f-6049-4095-ab27-fc5e79e789b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/SAP test\n",
      " Fake.csv\t\t      GoogleNews-vectors-negative300.bin.zip\n",
      "'fake news detection.ipynb'   True.csv\n",
      "'Final Draft.ipynb'\t     'Untitled document.gdoc'\n",
      " glove.twitter.27B.100d.txt\n"
     ]
    }
   ],
   "source": [
    "%cd  drive/MyDrive/SAP test/\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlrsvWNISRhU"
   },
   "outputs": [],
   "source": [
    "## Run this code to download the glove embedding file\n",
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile('GoogleNews-vectors-negative300.bin.zip', 'r')\n",
    "zip_ref.extractall('')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i8RlV9YKTRu4",
    "outputId": "036c4067-98b7-4496-a364-60e0d87249bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fake.csv\t\t      GoogleNews-vectors-negative300.bin\n",
      "'fake news detection.ipynb'   GoogleNews-vectors-negative300.bin.zip\n",
      "'Final Draft.ipynb'\t      True.csv\n",
      " glove.twitter.27B.100d.txt  'Untitled document.gdoc'\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uh0I_WBjMshG"
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "EMBEDDING_DIM=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUpZ41tbTb1V"
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "for word, vector in zip(word_vectors.vocab, word_vectors.vectors):\n",
    "  coefs = np.asarray(vector, dtype='float32')\n",
    "  embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcOLE6bITpJp"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word_index)+1\n",
    "embedding_matrix_w2v = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix_w2v[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix_w2v[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del word_vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ObcCSWsQTujT",
    "outputId": "b78f6d0f-750c-4a47-ec6c-fa583cd93273"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "##Training the Model\n",
    "#Defining Neural Network\n",
    "model_w2v = Sequential()\n",
    "#Non-trainable embeddidng layer\n",
    "model_w2v.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_matrix_w2v], input_length=maxlen, trainable=False))\n",
    "#LSTM \n",
    "model_w2v.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\n",
    "model_w2v.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\n",
    "model_w2v.add(Dense(units = 32 , activation = 'relu'))\n",
    "model_w2v.add(Dense(1, activation='sigmoid'))\n",
    "model_w2v.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['accuracy',f1_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YSsCU_Z8Tz3B",
    "outputId": "291d21d3-d375-43fa-8c0b-c3009140df44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 300, 300)          38178900  \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 300, 128)          219648    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 38,450,069\n",
      "Trainable params: 271,169\n",
      "Non-trainable params: 38,178,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_w2v.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CR4vTLAhT8SC",
    "outputId": "1d1cef2f-19db-4091-e400-9df4e10f18e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "158/158 [==============================] - 344s 2s/step - loss: 0.2944 - accuracy: 0.8701 - f1_m: 0.8567 - val_loss: 0.0416 - val_accuracy: 0.9889 - val_f1_m: 0.9876\n",
      "Epoch 2/5\n",
      "158/158 [==============================] - 341s 2s/step - loss: 0.0391 - accuracy: 0.9871 - f1_m: 0.9863 - val_loss: 0.0196 - val_accuracy: 0.9922 - val_f1_m: 0.9914\n",
      "Epoch 3/5\n",
      "158/158 [==============================] - 338s 2s/step - loss: 0.0214 - accuracy: 0.9934 - f1_m: 0.9930 - val_loss: 0.0101 - val_accuracy: 0.9969 - val_f1_m: 0.9966\n",
      "Epoch 4/5\n",
      "158/158 [==============================] - 335s 2s/step - loss: 0.0132 - accuracy: 0.9958 - f1_m: 0.9957 - val_loss: 0.0112 - val_accuracy: 0.9976 - val_f1_m: 0.9972\n",
      "Epoch 5/5\n",
      "158/158 [==============================] - 335s 2s/step - loss: 0.0066 - accuracy: 0.9981 - f1_m: 0.9980 - val_loss: 0.0073 - val_accuracy: 0.9982 - val_f1_m: 0.9979\n"
     ]
    }
   ],
   "source": [
    "history_w2v = model_w2v.fit(x_train_text, y_train, batch_size = batch_size , validation_data = (x_val_text,y_val) , epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xHXJJ5HeUKck",
    "outputId": "a9b5e33a-1a37-4e90-a5f4-a4db9ee56479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 19s 134ms/step - loss: 0.0073 - accuracy: 0.9982 - f1_m: 0.9981\n"
     ]
    }
   ],
   "source": [
    "_, accuracy_w2v, f1_score_w2v = model_w2v.evaluate(x=x_test_text, y= y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4zHin5-apU3",
    "outputId": "57b4a49c-c230-4d16-8e55-93f055ee4f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2Vec: Accuracy score   0.9982182383537292\n",
      "word2Vec: F1 score 0.9980553984642029\n"
     ]
    }
   ],
   "source": [
    "print(\"word2Vec: Accuracy score   {}\".format(accuracy_w2v))\n",
    "print(\"word2Vec: F1 score {}\".format(f1_score_w2v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXs_L_EEazUl"
   },
   "source": [
    "**Count Vectorization and Naive Baye**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8yuBn0Vb3sj",
    "outputId": "4ef0dc73-0241-4d9b-cd8a-9396968e56e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20107    progressive Hillary insults women victims sexu...\n",
       "7390     Mitch McConnell probably expecting Fox News ho...\n",
       "11612    MOSCOW (Reuters) - U.N. special envoy Syria St...\n",
       "20116    Communities like Ferguson, Baltimore Milwaukee...\n",
       "19686    NAYPYITAW (Reuters) - Myanmar leader Aung San ...\n",
       "                               ...                        \n",
       "7763     WINSTON-SALEM, N.C. (Reuters) - North Carolina...\n",
       "15377    MOSCOW (Reuters) - Kremlin aide said Wednesday...\n",
       "17730    TAMPA, Fla. (Reuters) - Defense Secretary Jim ...\n",
       "6613     April 29, Los Angeles County Superior Court Ju...\n",
       "15725    NEW YORK (Reuters) - wealthy Turkish gold trad...\n",
       "Name: text, Length: 40408, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsCpFUfzazBZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "vect = CountVectorizer()\n",
    "x_train_cv = vect.fit_transform(x_train)\n",
    "x_test_cv = vect.transform(x_test)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(x_train_cv,y_train)\n",
    "y_pred = nb.predict(x_test_cv)\n",
    "\n",
    "accuracy_cv, f1_score_cv = metrics.accuracy_score(y_test,y_pred), metrics.f1_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aZk93ok-buAO",
    "outputId": "619d06b6-79da-4423-c614-15eaf107d603"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizater: Accuracy score   0.9982182383537292\n",
      "CountVectorizater: F1 score 0.9980553984642029\n"
     ]
    }
   ],
   "source": [
    "print(\"CountVectorizater: Accuracy score   {}\".format(accuracy_w2v))\n",
    "print(\"CountVectorizater: F1 score {}\".format(f1_score_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlcGnfmPcK7f"
   },
   "source": [
    "# Summary\n",
    "\n",
    "Generally, across all model, the Accuracy and F1-score seems to be at the level of 99% and above. This might be due to the biasness in the dataset as pointed out in this [kaggle notebook.](https://www.kaggle.com/josutk/only-one-word-99-2)\n",
    "\n",
    "However, assuming that there is no bias in the dataset, it might be more ideal to use the **GloVe text-processing with the LSTM model.**  \n",
    "\n",
    "GloVe is preferred over CountVectorization because it uses a global dataset which reduces the chance of Out-of-Vocab problem and therefore it increases the sustainability and ease of maintainability in the future. Additionally, GloVe is able to capture semantic relationship between words. \n",
    "\n",
    "As compared to word2Vec, GloVe is also preferred as it uses a \"count-based\" model while word2Vec uses a \"predictive model\". \n",
    "\n",
    "Count-based model is easily parallelisable which makes it more ideal as it will be faster to train new model when the current model gets updated due to the evolving news content.  \n",
    "\n",
    "However, if dimensionality is an issue, word2vec will be preferred.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Final Draft.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
